#!/usr/bin/env node
/**
 * æ•°æ®åº“å¤‡ä»½è„šæœ¬
 * 
 * åŠŸèƒ½ï¼š
 * - æ”¯æŒå®Œæ•´å¤‡ä»½å’Œå¢é‡å¤‡ä»½
 * - è‡ªåŠ¨ä¸Šä¼ åˆ°è…¾è®¯äº‘ COS
 * - è‡ªåŠ¨æ¸…ç†è¿‡æœŸå¤‡ä»½
 * - å¤‡ä»½è®°å½•å­˜å‚¨åˆ°æ•°æ®åº“
 * - æ”¯æŒå®šæ—¶ä»»åŠ¡è°ƒç”¨
 * 
 * ä½¿ç”¨æ–¹æ³•ï¼š
 *   node backup-database.js                    # æ‰§è¡Œå®Œæ•´å¤‡ä»½
 *   node backup-database.js --type incremental # æ‰§è¡Œå¢é‡å¤‡ä»½
 *   node backup-database.js --cleanup          # æ¸…ç†è¿‡æœŸå¤‡ä»½
 *   node backup-database.js --list             # åˆ—å‡ºæ‰€æœ‰å¤‡ä»½
 *   node backup-database.js --no-upload        # å¤‡ä»½ä½†ä¸ä¸Šä¼ åˆ° COS
 */

import { exec } from 'child_process'
import { promisify } from 'util'
import fs from 'fs'
import path from 'path'
import { fileURLToPath } from 'url'
import dotenv from 'dotenv'
import pg from 'pg'

const execAsync = promisify(exec)
const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)

// åŠ è½½ç¯å¢ƒå˜é‡
dotenv.config({ path: path.join(__dirname, '../.env') })

// åŠ¨æ€å¯¼å…¥ COS å·¥å…·ï¼ˆé¿å…å¾ªç¯ä¾èµ–ï¼‰
let cosUploader = null
async function getCosUploader() {
  if (!cosUploader) {
    try {
      cosUploader = await import('../utils/cosUploader.js')
    } catch (error) {
      console.warn('âš ï¸ COS æ¨¡å—åŠ è½½å¤±è´¥ï¼Œå°†è·³è¿‡äº‘ç«¯ä¸Šä¼ :', error.message)
    }
  }
  return cosUploader
}

// é…ç½®
const config = {
  // å¤‡ä»½ç›®å½•
  backupDir: process.env.BACKUP_DIR || path.join(__dirname, '../backups'),
  // å¤‡ä»½ä¿ç•™å¤©æ•°
  retentionDays: parseInt(process.env.BACKUP_RETENTION_DAYS) || 30,
  // æœ€å¤§ä¿ç•™ä»½æ•°
  maxBackups: parseInt(process.env.BACKUP_MAX_COUNT) || 30,
  // æ•°æ®åº“è¿æ¥
  databaseUrl: process.env.DATABASE_URL_PROD || process.env.DATABASE_URL || process.env.DATABASE_URL_TEST
}

// é¢œè‰²è¾“å‡º
const colors = {
  reset: '\x1b[0m',
  green: '\x1b[32m',
  yellow: '\x1b[33m',
  red: '\x1b[31m',
  blue: '\x1b[34m',
  cyan: '\x1b[36m'
}

function log(message, color = 'reset') {
  console.log(`${colors[color]}${message}${colors.reset}`)
}

/**
 * è§£ææ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
 */
function parseDatabaseUrl(url) {
  try {
    const parsed = new URL(url)
    return {
      host: parsed.hostname,
      port: parsed.port || '5432',
      database: parsed.pathname.slice(1),
      user: parsed.username,
      password: parsed.password
    }
  } catch (error) {
    log('âŒ æ— æ³•è§£ææ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²', 'red')
    process.exit(1)
  }
}

/**
 * è·å–æ•°æ®åº“è¿æ¥
 */
async function getDbConnection() {
  const pool = new pg.Pool({
    connectionString: config.databaseUrl,
    ssl: config.databaseUrl.includes('localhost') ? false : { rejectUnauthorized: false }
  })
  return pool
}

/**
 * ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
 */
function ensureBackupDir() {
  if (!fs.existsSync(config.backupDir)) {
    fs.mkdirSync(config.backupDir, { recursive: true })
    log(`ğŸ“ åˆ›å»ºå¤‡ä»½ç›®å½•: ${config.backupDir}`, 'blue')
  }
}

/**
 * ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
 */
function generateBackupFileName(type = 'full') {
  const now = new Date()
  const timestamp = now.toISOString()
    .replace(/[-:]/g, '')
    .replace('T', '_')
    .split('.')[0]
  return `backup_${type}_${timestamp}.sql`
}

/**
 * è®°å½•å¤‡ä»½åˆ°æ•°æ®åº“
 */
async function recordBackup(pool, backupInfo) {
  try {
    const result = await pool.query(`
      INSERT INTO backup_records (
        backup_name, backup_type, backup_size, backup_path, backup_status,
        started_at, completed_at, error_message, created_by,
        cos_key, is_cloud_synced, file_name, description
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
      RETURNING id
    `, [
      backupInfo.backupName,
      backupInfo.backupType,
      backupInfo.backupSize,
      backupInfo.backupPath,
      backupInfo.status,
      backupInfo.startedAt,
      backupInfo.completedAt,
      backupInfo.errorMessage,
      backupInfo.createdBy || 'system',
      backupInfo.cosKey,
      backupInfo.isCloudSynced ? 1 : 0,
      backupInfo.fileName,
      backupInfo.description
    ])
    return result.rows[0]?.id
  } catch (error) {
    console.error('è®°å½•å¤‡ä»½ä¿¡æ¯å¤±è´¥:', error.message)
    return null
  }
}

/**
 * æ›´æ–°å¤‡ä»½è®°å½•
 */
async function updateBackupRecord(pool, id, updates) {
  try {
    const fields = []
    const values = []
    let paramIndex = 1

    for (const [key, value] of Object.entries(updates)) {
      const dbKey = key.replace(/([A-Z])/g, '_$1').toLowerCase()
      fields.push(`${dbKey} = $${paramIndex}`)
      values.push(value)
      paramIndex++
    }

    values.push(id)
    await pool.query(
      `UPDATE backup_records SET ${fields.join(', ')} WHERE id = $${paramIndex}`,
      values
    )
  } catch (error) {
    console.error('æ›´æ–°å¤‡ä»½è®°å½•å¤±è´¥:', error.message)
  }
}

/**
 * æ‰§è¡Œæ•°æ®åº“å¤‡ä»½
 */
async function performBackup(type = 'full', options = {}) {
  const startTime = Date.now()
  const startedAt = new Date()
  const dbConfig = parseDatabaseUrl(config.databaseUrl)
  const fileName = generateBackupFileName(type)
  const filePath = path.join(config.backupDir, fileName)
  const compressedFileName = `${fileName}.gz`
  const compressedPath = `${filePath}.gz`
  
  const pool = await getDbConnection()
  let backupId = null
  
  log('')
  log('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—', 'cyan')
  log('â•‘       ğŸ“¦ PostgreSQL æ•°æ®åº“å¤‡ä»½å·¥å…· (COS äº‘å­˜å‚¨ç‰ˆ)           â•‘', 'cyan')
  log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•', 'cyan')
  log('')
  log(`â° å¼€å§‹æ—¶é—´: ${startedAt.toLocaleString('zh-CN')}`)
  log(`ğŸ“ å¤‡ä»½ç±»å‹: ${type === 'full' ? 'å®Œæ•´å¤‡ä»½' : 'å¢é‡å¤‡ä»½'}`)
  log(`ğŸ—„ï¸  æ•°æ®åº“: ${dbConfig.database}`)
  log(`ğŸ“‚ å¤‡ä»½è·¯å¾„: ${filePath}`)
  log('')
  
  try {
    // åˆ›å»ºåˆå§‹å¤‡ä»½è®°å½•
    backupId = await recordBackup(pool, {
      backupName: `${type}_backup_${startedAt.toISOString().split('T')[0]}`,
      backupType: type,
      backupSize: 0,
      backupPath: compressedPath,
      status: 'running',
      startedAt: startedAt,
      completedAt: null,
      errorMessage: null,
      fileName: compressedFileName,
      description: `${type === 'full' ? 'å®Œæ•´' : 'å¢é‡'}æ•°æ®åº“å¤‡ä»½`,
      cosKey: null,
      isCloudSynced: false
    })
    
    // è®¾ç½® PGPASSWORD ç¯å¢ƒå˜é‡
    process.env.PGPASSWORD = dbConfig.password
    
    // æ„å»º pg_dump å‘½ä»¤
    let command = `pg_dump -h ${dbConfig.host} -p ${dbConfig.port} -U ${dbConfig.user} -d ${dbConfig.database}`
    
    if (type === 'full') {
      // å®Œæ•´å¤‡ä»½ï¼šåŒ…å«æ‰€æœ‰æ•°æ®
      command += ' --format=plain --no-owner --no-acl'
    } else {
      // å¢é‡å¤‡ä»½ï¼šåªå¤‡ä»½æ•°æ®ï¼ˆä¸å«ç»“æ„ï¼‰
      command += ' --format=plain --no-owner --no-acl --data-only'
    }
    
    command += ` > "${filePath}"`
    
    log('ğŸ”„ æ­£åœ¨æ‰§è¡Œå¤‡ä»½...', 'yellow')
    
    await execAsync(command, {
      env: { ...process.env, PGPASSWORD: dbConfig.password },
      maxBuffer: 1024 * 1024 * 100 // 100MB buffer
    })
    
    // è·å–æ–‡ä»¶å¤§å°
    const stats = fs.statSync(filePath)
    const sizeInMB = (stats.size / (1024 * 1024)).toFixed(2)
    
    log('ğŸ—œï¸  æ­£åœ¨å‹ç¼©å¤‡ä»½æ–‡ä»¶...', 'yellow')
    
    // å‹ç¼©å¤‡ä»½æ–‡ä»¶
    await execAsync(`gzip -9 "${filePath}"`)
    
    const compressedStats = fs.statSync(compressedPath)
    const compressedSizeInMB = (compressedStats.size / (1024 * 1024)).toFixed(2)
    
    log('')
    log('âœ… æœ¬åœ°å¤‡ä»½å®Œæˆï¼', 'green')
    log(`   ğŸ“„ æ–‡ä»¶å: ${compressedFileName}`)
    log(`   ğŸ“Š åŸå§‹å¤§å°: ${sizeInMB} MB`)
    log(`   ğŸ“¦ å‹ç¼©å: ${compressedSizeInMB} MB`)
    log('')
    
    // ä¸Šä¼ åˆ° COSï¼ˆå¦‚æœé…ç½®äº†ä¸”æœªç¦ç”¨ï¼‰
    let cosKey = null
    let isCloudSynced = false
    
    if (!options.noUpload) {
      const cos = await getCosUploader()
      if (cos && cos.isCosConfigured && cos.isCosConfigured()) {
        try {
          log('â˜ï¸  æ­£åœ¨ä¸Šä¼ åˆ°è…¾è®¯äº‘ COS...', 'yellow')
          const uploadResult = await cos.uploadFile(compressedPath)
          cosKey = uploadResult.key
          isCloudSynced = true
          log(`âœ… äº‘ç«¯ä¸Šä¼ æˆåŠŸ: ${cosKey}`, 'green')
        } catch (uploadError) {
          log(`âš ï¸ äº‘ç«¯ä¸Šä¼ å¤±è´¥: ${uploadError.message}`, 'yellow')
          log('   å¤‡ä»½æ–‡ä»¶å·²ä¿å­˜åˆ°æœ¬åœ°', 'yellow')
        }
      } else {
        log('â„¹ï¸  æœªé…ç½®è…¾è®¯äº‘ COSï¼Œè·³è¿‡äº‘ç«¯ä¸Šä¼ ', 'blue')
      }
    } else {
      log('â„¹ï¸  å·²ç¦ç”¨äº‘ç«¯ä¸Šä¼ ', 'blue')
    }
    
    const completedAt = new Date()
    const duration = ((Date.now() - startTime) / 1000).toFixed(2)
    
    // æ›´æ–°å¤‡ä»½è®°å½•
    if (backupId) {
      await updateBackupRecord(pool, backupId, {
        backupSize: compressedStats.size,
        backupStatus: 'completed',
        completedAt: completedAt,
        cosKey: cosKey,
        isCloudSynced: isCloudSynced ? 1 : 0
      })
    }
    
    log('')
    log('â•'.repeat(60), 'cyan')
    log(`â±ï¸  æ€»è€—æ—¶: ${duration} ç§’`)
    log(`ğŸ’¾ æœ¬åœ°è·¯å¾„: ${compressedPath}`)
    if (cosKey) {
      log(`â˜ï¸  äº‘ç«¯è·¯å¾„: ${cosKey}`)
    }
    log('â•'.repeat(60), 'cyan')
    log('')
    
    // è®°å½•å¤‡ä»½ä¿¡æ¯åˆ°æ—¥å¿—æ–‡ä»¶
    const logEntry = {
      timestamp: new Date().toISOString(),
      type,
      fileName: compressedFileName,
      originalSize: stats.size,
      compressedSize: compressedStats.size,
      duration: parseFloat(duration),
      cosKey,
      isCloudSynced,
      status: 'success'
    }
    
    appendToBackupLog(logEntry)
    
    await pool.end()
    
    return {
      success: true,
      backupId,
      fileName: compressedFileName,
      filePath: compressedPath,
      size: compressedStats.size,
      duration: parseFloat(duration),
      cosKey,
      isCloudSynced
    }
    
  } catch (error) {
    log('')
    log('âŒ å¤‡ä»½å¤±è´¥ï¼', 'red')
    log(`   é”™è¯¯ä¿¡æ¯: ${error.message}`, 'red')
    log('')
    
    // æ¸…ç†å¯èƒ½å­˜åœ¨çš„ä¸å®Œæ•´æ–‡ä»¶
    if (fs.existsSync(filePath)) {
      fs.unlinkSync(filePath)
    }
    if (fs.existsSync(compressedPath)) {
      fs.unlinkSync(compressedPath)
    }
    
    // æ›´æ–°å¤‡ä»½è®°å½•çŠ¶æ€
    if (backupId) {
      await updateBackupRecord(pool, backupId, {
        backupStatus: 'failed',
        completedAt: new Date(),
        errorMessage: error.message
      })
    }
    
    appendToBackupLog({
      timestamp: new Date().toISOString(),
      type,
      status: 'failed',
      error: error.message
    })
    
    await pool.end()
    
    return {
      success: false,
      error: error.message
    }
  }
}

/**
 * æ¸…ç†è¿‡æœŸå¤‡ä»½
 */
async function cleanupOldBackups() {
  log('')
  log('ğŸ§¹ å¼€å§‹æ¸…ç†è¿‡æœŸå¤‡ä»½...', 'yellow')
  
  const pool = await getDbConnection()
  const cos = await getCosUploader()
  
  // æœ¬åœ°æ–‡ä»¶æ¸…ç†
  const files = fs.readdirSync(config.backupDir)
    .filter(f => f.startsWith('backup_') && f.endsWith('.gz'))
    .map(f => ({
      name: f,
      path: path.join(config.backupDir, f),
      mtime: fs.statSync(path.join(config.backupDir, f)).mtime
    }))
    .sort((a, b) => b.mtime - a.mtime) // æŒ‰ä¿®æ”¹æ—¶é—´å€’åº
  
  const cutoffDate = new Date()
  cutoffDate.setDate(cutoffDate.getDate() - config.retentionDays)
  
  let deletedCount = 0
  let deletedSize = 0
  const deletedCosKeys = []
  
  // åˆ é™¤è¶…è¿‡ä¿ç•™å¤©æ•°çš„å¤‡ä»½
  for (const file of files) {
    if (file.mtime < cutoffDate) {
      const stats = fs.statSync(file.path)
      fs.unlinkSync(file.path)
      deletedCount++
      deletedSize += stats.size
      log(`   ğŸ—‘ï¸  åˆ é™¤æœ¬åœ°: ${file.name}`, 'yellow')
      
      // æŸ¥æ‰¾å¯¹åº”çš„ COS æ–‡ä»¶
      try {
        const record = await pool.query(
          'SELECT cos_key FROM backup_records WHERE file_name = $1 AND cos_key IS NOT NULL',
          [file.name]
        )
        if (record.rows[0]?.cos_key) {
          deletedCosKeys.push(record.rows[0].cos_key)
        }
      } catch (e) {
        // å¿½ç•¥æŸ¥è¯¢é”™è¯¯
      }
    }
  }
  
  // å¦‚æœå¤‡ä»½æ•°é‡è¶…è¿‡æœ€å¤§å€¼ï¼Œåˆ é™¤æœ€è€çš„
  const remainingFiles = files.filter(f => fs.existsSync(f.path))
  if (remainingFiles.length > config.maxBackups) {
    const toDelete = remainingFiles.slice(config.maxBackups)
    for (const file of toDelete) {
      if (fs.existsSync(file.path)) {
        const stats = fs.statSync(file.path)
        fs.unlinkSync(file.path)
        deletedCount++
        deletedSize += stats.size
        log(`   ğŸ—‘ï¸  åˆ é™¤ï¼ˆè¶…å‡ºæ•°é‡é™åˆ¶ï¼‰: ${file.name}`, 'yellow')
        
        // æŸ¥æ‰¾å¯¹åº”çš„ COS æ–‡ä»¶
        try {
          const record = await pool.query(
            'SELECT cos_key FROM backup_records WHERE file_name = $1 AND cos_key IS NOT NULL',
            [file.name]
          )
          if (record.rows[0]?.cos_key) {
            deletedCosKeys.push(record.rows[0].cos_key)
          }
        } catch (e) {
          // å¿½ç•¥æŸ¥è¯¢é”™è¯¯
        }
      }
    }
  }
  
  // åˆ é™¤ COS ä¸Šçš„æ–‡ä»¶
  if (deletedCosKeys.length > 0 && cos && cos.isCosConfigured && cos.isCosConfigured()) {
    try {
      log(`   â˜ï¸  åˆ é™¤äº‘ç«¯æ–‡ä»¶ ${deletedCosKeys.length} ä¸ª...`, 'yellow')
      await cos.deleteFiles(deletedCosKeys)
    } catch (e) {
      log(`   âš ï¸ äº‘ç«¯æ–‡ä»¶åˆ é™¤å¤±è´¥: ${e.message}`, 'yellow')
    }
  }
  
  // æ›´æ–°æ•°æ®åº“è®°å½•
  try {
    await pool.query(
      `DELETE FROM backup_records WHERE created_at < $1`,
      [cutoffDate]
    )
  } catch (e) {
    // å¿½ç•¥åˆ é™¤é”™è¯¯
  }
  
  await pool.end()
  
  if (deletedCount > 0) {
    const freedMB = (deletedSize / (1024 * 1024)).toFixed(2)
    log('')
    log(`âœ… æ¸…ç†å®Œæˆï¼šåˆ é™¤ ${deletedCount} ä¸ªæ–‡ä»¶ï¼Œé‡Šæ”¾ ${freedMB} MB ç©ºé—´`, 'green')
  } else {
    log('   æ²¡æœ‰éœ€è¦æ¸…ç†çš„å¤‡ä»½æ–‡ä»¶', 'blue')
  }
  log('')
}

/**
 * åˆ—å‡ºæ‰€æœ‰å¤‡ä»½
 */
async function listBackups() {
  log('')
  log('ğŸ“‹ å¤‡ä»½æ–‡ä»¶åˆ—è¡¨', 'blue')
  log('â•'.repeat(80))
  
  const pool = await getDbConnection()
  
  try {
    // ä»æ•°æ®åº“è·å–å¤‡ä»½è®°å½•
    const result = await pool.query(`
      SELECT id, backup_name, backup_type, backup_size, backup_status,
             cos_key, is_cloud_synced, file_name, created_at
      FROM backup_records
      ORDER BY created_at DESC
      LIMIT 50
    `)
    
    if (result.rows.length === 0) {
      log('   æš‚æ— å¤‡ä»½è®°å½•', 'yellow')
    } else {
      log('')
      log('ID   | æ–‡ä»¶å                              | å¤§å°      | çŠ¶æ€     | äº‘åŒæ­¥ | æ—¶é—´', 'blue')
      log('-'.repeat(100))
      
      for (const record of result.rows) {
        const sizeMB = record.backup_size ? (record.backup_size / (1024 * 1024)).toFixed(2) + ' MB' : 'N/A'
        const date = new Date(record.created_at).toLocaleString('zh-CN')
        const cloudStatus = record.is_cloud_synced ? 'âœ“' : 'âœ—'
        const status = record.backup_status === 'completed' ? 'å®Œæˆ' : 
                      record.backup_status === 'running' ? 'è¿›è¡Œä¸­' : 'å¤±è´¥'
        
        log(`${String(record.id).padEnd(4)} | ${(record.file_name || record.backup_name).padEnd(35)} | ${sizeMB.padEnd(9)} | ${status.padEnd(8)} | ${cloudStatus.padEnd(6)} | ${date}`)
      }
      
      log('-'.repeat(100))
      
      const totalSize = result.rows.reduce((sum, r) => sum + (r.backup_size || 0), 0)
      const cloudCount = result.rows.filter(r => r.is_cloud_synced).length
      log(`å…± ${result.rows.length} æ¡è®°å½•ï¼Œæ€»è®¡ ${(totalSize / (1024 * 1024)).toFixed(2)} MBï¼Œ${cloudCount} ä¸ªå·²åŒæ­¥åˆ°äº‘ç«¯`)
    }
  } catch (error) {
    log(`   è·å–å¤‡ä»½è®°å½•å¤±è´¥: ${error.message}`, 'red')
    
    // é™çº§ï¼šæ˜¾ç¤ºæœ¬åœ°æ–‡ä»¶
    log('')
    log('   æœ¬åœ°æ–‡ä»¶åˆ—è¡¨:', 'yellow')
    const files = fs.readdirSync(config.backupDir)
      .filter(f => f.startsWith('backup_') && f.endsWith('.gz'))
      .map(f => {
        const stats = fs.statSync(path.join(config.backupDir, f))
        return {
          name: f,
          size: stats.size,
          mtime: stats.mtime
        }
      })
      .sort((a, b) => b.mtime - a.mtime)
    
    if (files.length === 0) {
      log('   æš‚æ— æœ¬åœ°å¤‡ä»½æ–‡ä»¶', 'yellow')
    } else {
      for (const file of files) {
        const sizeMB = (file.size / (1024 * 1024)).toFixed(2)
        const date = file.mtime.toLocaleString('zh-CN')
        log(`   ${file.name}  |  ${sizeMB} MB  |  ${date}`)
      }
    }
  }
  
  await pool.end()
  log('')
}

/**
 * è¿½åŠ å¤‡ä»½æ—¥å¿—
 */
function appendToBackupLog(entry) {
  const logFile = path.join(config.backupDir, 'backup.log')
  const logLine = JSON.stringify(entry) + '\n'
  fs.appendFileSync(logFile, logLine)
}

/**
 * ä¸»å‡½æ•°
 */
async function main() {
  const args = process.argv.slice(2)
  
  // æ£€æŸ¥æ•°æ®åº“è¿æ¥é…ç½®
  if (!config.databaseUrl) {
    log('âŒ é”™è¯¯ï¼šæœªé…ç½®æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²', 'red')
    log('   è¯·è®¾ç½® DATABASE_URL_PROD æˆ– DATABASE_URL ç¯å¢ƒå˜é‡', 'yellow')
    process.exit(1)
  }
  
  // ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
  ensureBackupDir()
  
  // è§£æå‘½ä»¤è¡Œå‚æ•°
  if (args.includes('--list')) {
    await listBackups()
  } else if (args.includes('--cleanup')) {
    await cleanupOldBackups()
  } else {
    const type = args.includes('--type') 
      ? args[args.indexOf('--type') + 1] || 'full'
      : 'full'
    
    const noUpload = args.includes('--no-upload')
    
    const result = await performBackup(type, { noUpload })
    
    if (result.success) {
      // å¤‡ä»½æˆåŠŸåè‡ªåŠ¨æ¸…ç†
      await cleanupOldBackups()
    }
    
    process.exit(result.success ? 0 : 1)
  }
}

// å¯¼å‡ºå‡½æ•°ä¾›å…¶ä»–æ¨¡å—è°ƒç”¨
export { performBackup, cleanupOldBackups, listBackups }

// è¿è¡Œ
main().catch(error => {
  log(`âŒ æ‰§è¡Œé”™è¯¯: ${error.message}`, 'red')
  process.exit(1)
})
